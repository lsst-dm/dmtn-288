\section{Introduction}

The Vera C.\ Rubin Observatory is located on Cerro Pachon in Chile and consists of the 8.4\,m Simonyi Survey Telescope and the 1.2\,m Rubin Auxiliary Telescope.
Over the course of 10 years the Rubin Observatory will take data for the Legacy Survey of Space and Time (LSST) and deliver 11 data releases \cite{2019ApJ...873..111I}.
The Rubin Observatory generates a data volume of approximately 20\,TB per night and this data is handled using a Data Management System \cite{2022arXiv221113611O} that transfers the files from the summit to the US Data Facility hosted at the SLAC National Accelerator Laboratory (SLAC) and triggers the pipeline processing and makes the data available to the data access centers.
The LSST Science Pipelines \cite{2019ASPC..523..521B} will be used to process the data and are designed such that neither the pipeline algorithmic code nor the scientist inspecting the data know where the data are stored or what data format they are stored in.
The software abstraction layer we use for this is called the Data Butler. \cite{2019ASPC..523..653J,2022SPIE12189E..11J}

\section{The Data Butler}

The Rubin Observatory Data Butler concept has been in the data management plan from the very early days of the project \cite{2007ASPC..376....3K,2010SPIE.7740E..15A}.
In 2017 \cite{LDM-563} we decided to rewrite the Butler library from scratch to take into account new technologies, clarified requirements, \cite{LDM-556} and lessons learned from previous implementations.
This version, colloquially known as the ``generation 3 middleware'', was formally accepted by the project in 2022. \cite{DMTR-271}

The Data Butler has been described in a previous paper \cite{2022SPIE12189E..11J} but the important concepts of dimension records, data coordinates, collections, dataset types, and formatters are summarized here.

\subsection{Dimension Records and Coordinates}

The Butler registry assigns datasets specific coordinates within a dimensional space (what we call a ``dimension universe'').
These dimensions generally refer to scientifically useful concepts such as the instrument, an observation on the sky, or a patch on the sky.
Some common examples of dimensions used at the Rubin Observatory are listed in Table\,\ref{tab:dimensions}.
These dimensions and associated coordinates (which can only be strings or integers) are created when new observations are ingested or when new instruments or skymaps are defined.

\begin{table}
\begin{tabular}{ll}
\texttt{instrument} &  The instrument that generated this data. \\
\texttt{band} & Waveband of interest.  \\
\texttt{detector} & A specific detector within the instrument. \\
\texttt{physical\_filter} &  Filter used for the exposure. \\
\texttt{day\_obs} & The observing day. \\
\texttt{group} &  Group identifier. \\
\texttt{exposure} & Individual exposure. \\
\texttt{visit} &  Collection of 1 or 2 exposures. \\
\texttt{tract} &  Tesselation of the sky. \\
\texttt{patch} &  Patch within a tract.\\
\end{tabular}
\caption{Common dimensions present in the default dimension universe.}
\label{tab:dimensions}
\end{table}

\subsection{Dataset Types}

A ``dataset type'' describes an input or an output from a data processing task in a general way.
The name of the dataset type is meaningful to the pipeline user or scientist with examples such as ``raw'' (a file that comes straight from the instrument), ``calexp'' (a calibrated single exposure), or ``coadd'' (a generic stack of multiple images).
The dataset type also specifies the relevant dimensions -- a ``raw'' might be described by \texttt{exposure}, \texttt{instrument}, \texttt{detector} but an output co-add might be described by \texttt{tract}, \texttt{patch}, \texttt{skymap}, and \texttt{band}.

The final part of a dataset type is the ``storage class''.
The storage class is a proxy for the Python type that will be used.
Along with the Python type it can also describe individual components (such as metadata or image, variance, or mask) which are declared to be retrievable independently, and also specify conversion methods to handle cases where a pipeline task returns something that is compatible with the underlying storage class but is not identical.

\subsection{Collections}

Every dataset in a Butler repository must be stored in a \texttt{RUN} collection.
This collection can be thought of as representing a folder or directory, although there is no requirement that a file will be written somewhere.
A single \texttt{RUN} collection can only contain one dataset with a dataset type and data coordinate combination.

A \texttt{CHAINED} collection consists of a list of collections that will be searched in order.
These collections can contain more \texttt{CHAINED} collections.
A \texttt{CHAINED} collection is usually created during a data processing campaign and generally consists of all the input collections and a timestamped collection.
This allows a single output collection to be used to find all the inputs and outputs and the timestamped name allows resubmissions to be stacked using additional timestamped names.

A \texttt{TAGGED} collection is used to create a bespoke collection containing a curated list of datasets.
The only requirement is that there are no duplicate dataset type / data coordinate combinations.

A \texttt{CALIBRATION} collection is a special type of collection that does allow duplicated dataset type and data coordinate combinations but uses validity timespans to break any ambiguity.
This allows a raw exposure, which is implicitly anchored in time, to be associated with the relevant processed calibration datasets.

\subsection{Formatters}

A Butler datastore enables provides an additional abstraction layer within Butler that decouples the serialization and storage of a dataset from knowledge of its existence.
Datastores exist that can write directly to the Rubin Observatory metrics database \cite{2024SPIE13101.59Ftmp} and talk to multiple datastores as if they were one datastore, but the most commonly-used datastore implementation is one that reads and writes files.
When a file-backed datastore is given a Python object it uses a Python formatter class to serialize it to bytes and, conversely, when someone requests a dataset the datastore reads the bytes and uses the formatter to convert it to a Python object.
The choice of formatter class is controlled by datastore configuration and is selected by comparing with the storage class or dataset type.

\section{Moving to the Cloud}

The original operating plan for the LSST was to host the data at the National Center for Supercomputing Applications in Champaign-Urbana. \cite{2012SPIE.8451E..0VF}
In this scenario the 10,000 science users that the archive was sized to support would all be given accounts directly at the archive center and access would be controlled directly on the file system using ACLs and on the databases using database accounts.
With the move to SLAC as the archive center \cite{RTN-021} this approach was no longer feasible given that SLAC is a Department of Energy facility which has much tighter controls over who can be issued computer accounts and a much more detailed background check.
Ten thousand accounts would take a significant amount of time to be processed and it was unacceptable to the project that some science users with data rights might not be able to acquire accounts at all.

To solve this problem the project adopted a ``hybrid cloud'' solution \cite{2024SPIE13101.86Otmp} where all the science users will be using the Rubin Science Platform \cite{LDM-542} and be hosted on a commercial cloud where they will be given access via their educational instititution's accounts, but the data would be stored at the US Data Facility at SLAC.
In this way the DOE would not need to vet 10,000 users and we would proxy access through our infrastructure.
We prototyped cloud hosting on Google using one of our Data Previews \cite{2021arXiv211115030O} and demonstrated that the deployment, data access, and user support would work correctly with a few hundred science users.
The Data Previews, though, stored the data in the cloud and used shared authentication credentials for data access -- for LSST Data Releases the data volume will be too large to host on the cloud, with our current budget profile, and we are required to provide per-user and per-group access controls.

To support this hybrid cloud approach and to provide scalability for 10,000 users on day one of a data release, we were required to rethink our approach to the Butler and re-engineer it to use a client/server architecture.
In this way the server can use standard Rubin Science Platform authentication, \cite{DMTN-182} utilize signed URLs to access the data at the USDF without requiring SLAC accounts, \cite{DMTN-284} and allow for a backend infrastructure that can make use of cloud resources to scale with spikes in load.

\section{Migrating to a Client/Server Architecture}

The delivered Butler uses a direct connection to a database (generally PostgreSQL) and direct access to either a POSIX file system, an object store (supporting either Google Cloud Storage or Amazon S3), or a WebDAV server.
There is no fine-grained access control restricting database access and object store access is generally all or nothing.
For a POSIX file system datastore it is possible to restrict file system access based on ACLs.
We call this implementation the ``direct butler'' to distinguish it from the ``remote butler'' that connects to a butler server.

For Data Preview 0.2 \cite{RTN-041} this was the system we used -- anybody could read and write anything with no group or user restrictions.
For a deployment that was mainly used as a test bed and which was using previously published simulated data this was not an issue.
This approach to authentication and authorization is acceptable when running batch processing at the USDF where the campaign processing team and developers are Rubin Observatory staff, but a new approach is needed for the hybrid cloud scenario with external science users for formal data releases.

For a formal data release or access to the prompt products database, the requirements become stricter:

\begin{itemize}
\item Data products are only visible to data rights holders until the two-year proprietary period is exceeded.
\item Data rights holders can all see the formal data products but can not delete or modify them and can not write to any of the data release collections.
\item Data rights holders should be allowed to store their own derived products and no-one else should be able to access them or even know they exist.
\item Data rights holders should be allowed to collaborate with others and make use of shared collections that are private to their group.
\item File quotas will be used to ensure fairness, although a petition can be made to increase the default quota.
\end{itemize}

\section{Conclusions}

Converting an existing library that uses direct database and object store connections to one which uses modern web service paradigms, including supporting scalability and security, is a challenging exercise.
Whilst we realized early on that such a change was needed \cite{DMTN-169,DMTN-176} other priorities and staff resourcing issues meant that we could not really begin to look seriously at the implementation until more recently. \cite{DMTN-282,DMTN-283}

We have made significant progress in the implementation of the client/server Butler this year and it is already deployed with core functionality and used by the Rubin Science Platform Virtual Observatory services. \cite{DMTN-208}
A system will be deployed in time for Data Preview 1 in early 2025. \cite{RTN-011}
